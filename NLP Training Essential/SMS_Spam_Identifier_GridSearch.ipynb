{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Classifiers: Random Forest on a holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...   8094  8095  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "#Count Vectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text'])\n",
    "X_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "X_count_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore RandomForestClassifier through Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, \n",
    "                            n_estimators=50,\n",
    "                            max_depth=20\n",
    "                           )\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.05199014537757954, 1803),\n",
       " (0.04090862832384518, 'body_len'),\n",
       " (0.03366259982109352, 3134),\n",
       " (0.02317670205873484, 7350),\n",
       " (0.022976561813771886, 4796),\n",
       " (0.021577449153920965, 5724),\n",
       " (0.017855200989950297, 2031),\n",
       " (0.017608404529609704, 6746),\n",
       " (0.017399681400886158, 7782),\n",
       " (0.016451760352450998, 2171),\n",
       " (0.01550982909218497, 1361),\n",
       " (0.014165421141110328, 397),\n",
       " (0.013944865658424272, 295),\n",
       " (0.012491788420677528, 690),\n",
       " (0.010939566006462571, 392),\n",
       " (0.010861086701957824, 5988),\n",
       " (0.010834128792908564, 436),\n",
       " (0.010626087980274514, 7543),\n",
       " (0.010613676886963361, 2299),\n",
       " (0.010583847087089872, 6285),\n",
       " (0.010553973015936908, 5453),\n",
       " (0.010042824688850573, 3443),\n",
       " (0.00945980953128109, 354),\n",
       " (0.009332151670672934, 294),\n",
       " (0.008971483030548576, 7218),\n",
       " (0.008664347120273343, 611),\n",
       " (0.00860909047881913, 5078),\n",
       " (0.008513668570590233, 7590),\n",
       " (0.008277837484125502, 1941),\n",
       " (0.008189672997723265, 4269),\n",
       " (0.007609300380908793, 7461),\n",
       " (0.0073615669869203605, 7027),\n",
       " (0.006994545572171262, 1881),\n",
       " (0.006944482179878205, 5875),\n",
       " (0.00678125872945611, 1832),\n",
       " (0.006221670344190864, 1932),\n",
       " (0.006147455346645382, 2095),\n",
       " (0.005901497975697989, 'punct%'),\n",
       " (0.00588376750283768, 4733),\n",
       " (0.005704330464013569, 5917),\n",
       " (0.005605191784176824, 5005),\n",
       " (0.005567290925533683, 5568),\n",
       " (0.00548691358761088, 813),\n",
       " (0.005073285607837411, 691),\n",
       " (0.005020762850271804, 873),\n",
       " (0.004911731295565213, 4378),\n",
       " (0.004698550715085571, 1228),\n",
       " (0.004671617748002086, 7317),\n",
       " (0.004580815974477343, 4793),\n",
       " (0.0045105413287267835, 439),\n",
       " (0.004365765336910629, 6050),\n",
       " (0.0043458028476075885, 1359),\n",
       " (0.004265198075560249, 7379),\n",
       " (0.004075234317893188, 5287),\n",
       " (0.004034566803120259, 4492),\n",
       " (0.004006716238470278, 7436),\n",
       " (0.003975360647639878, 6971),\n",
       " (0.003931352600134014, 4999),\n",
       " (0.0038451955876993103, 360),\n",
       " (0.003814768177952613, 2619),\n",
       " (0.003771967668769396, 50),\n",
       " (0.0036555886289664054, 368),\n",
       " (0.0036278464475161553, 2204),\n",
       " (0.0035180382069379813, 375),\n",
       " (0.0034769478225222506, 6251),\n",
       " (0.003464738558348985, 6130),\n",
       " (0.003422404332486843, 2790),\n",
       " (0.003400147187767072, 7239),\n",
       " (0.003318020912023382, 1320),\n",
       " (0.003245684501562597, 3665),\n",
       " (0.003069910383998487, 788),\n",
       " (0.0030436212759494107, 2134),\n",
       " (0.0029988826847846895, 2081),\n",
       " (0.002927476978153721, 5596),\n",
       " (0.0029005200284384115, 441),\n",
       " (0.0028553682758052827, 7506),\n",
       " (0.0027822592833300196, 4268),\n",
       " (0.0027387283719057676, 7696),\n",
       " (0.002696123169331313, 7355),\n",
       " (0.0026931213215561143, 5278),\n",
       " (0.0026393081751147607, 1599),\n",
       " (0.002551501207392946, 6485),\n",
       " (0.002532044915188222, 470),\n",
       " (0.0024938225742800564, 7693),\n",
       " (0.0024910128438516616, 6377),\n",
       " (0.002452757365021598, 7429),\n",
       " (0.002430457222916226, 2426),\n",
       " (0.0024276486434623894, 1925),\n",
       " (0.0024014041004881788, 4295),\n",
       " (0.002251808378227888, 7363),\n",
       " (0.0022467018068418527, 53),\n",
       " (0.002229569897979718, 1649),\n",
       " (0.002225600106740451, 6810),\n",
       " (0.0021988621873604425, 7816),\n",
       " (0.002157068173489994, 438),\n",
       " (0.002103702157592824, 2499),\n",
       " (0.0021027033222129933, 5835),\n",
       " (0.002076423572339316, 5196),\n",
       " (0.0020633910786596564, 7645),\n",
       " (0.00204711124723298, 739),\n",
       " (0.0020139283139163906, 338),\n",
       " (0.0019744334734149243, 2784),\n",
       " (0.0019257187275150544, 324),\n",
       " (0.0019240414098578957, 7458),\n",
       " (0.0019139588357383532, 816),\n",
       " (0.0018620953857901427, 876),\n",
       " (0.001845585341527226, 5993),\n",
       " (0.001837300786830601, 7789),\n",
       " (0.0018126373330863421, 5551),\n",
       " (0.0017607878522193907, 6235),\n",
       " (0.001736816190594805, 54),\n",
       " (0.0017312542108009242, 1620),\n",
       " (0.0017236874596231954, 571),\n",
       " (0.001712859086896347, 5291),\n",
       " (0.0016937888219604232, 2600),\n",
       " (0.0016801174063932455, 3706),\n",
       " (0.0016775628873708381, 3142),\n",
       " (0.001672089941516502, 5721),\n",
       " (0.001635115081313441, 2161),\n",
       " (0.0016282644576437384, 3824),\n",
       " (0.001614550050008573, 2890),\n",
       " (0.0015837109687116767, 3933),\n",
       " (0.0015829137549743294, 2720),\n",
       " (0.0015817760081495782, 4623),\n",
       " (0.0015691758924042348, 4496),\n",
       " (0.001556916428501375, 7868),\n",
       " (0.0015539285625086616, 425),\n",
       " (0.0015407753266353035, 3011),\n",
       " (0.0015195301592805363, 2293),\n",
       " (0.0014539433984761658, 4678),\n",
       " (0.0014463119081973977, 3827),\n",
       " (0.0014349494221269582, 1018),\n",
       " (0.0013607848828177693, 1549),\n",
       " (0.0013534704126361102, 3743),\n",
       " (0.0013473781270840978, 3320),\n",
       " (0.0013220229807993657, 4799),\n",
       " (0.0012756463491010147, 3943),\n",
       " (0.0011810603722472914, 293),\n",
       " (0.0011717185352057738, 2060),\n",
       " (0.0011654394401961296, 6262),\n",
       " (0.0011364219847617, 2099),\n",
       " (0.0011273725434040591, 4850),\n",
       " (0.0011209334131873114, 5888),\n",
       " (0.0011162891513632654, 5269),\n",
       " (0.0011021007323601035, 2297),\n",
       " (0.001098183623545926, 7010),\n",
       " (0.001093738075840807, 3807),\n",
       " (0.0010685279359463535, 118),\n",
       " (0.0010643131327290319, 4308),\n",
       " (0.001063367953667958, 3855),\n",
       " (0.0010532976021174966, 2351),\n",
       " (0.0010320237915074782, 4209),\n",
       " (0.0010165933364857619, 5643),\n",
       " (0.0010142278997432906, 2268),\n",
       " (0.0010130425782277162, 3291),\n",
       " (0.0010032026580414344, 7508),\n",
       " (0.0009773214202154323, 4701),\n",
       " (0.0009634815908576918, 5098),\n",
       " (0.0009598212897339628, 4953),\n",
       " (0.0009595978085260994, 1897),\n",
       " (0.0009480949791792604, 3880),\n",
       " (0.0009420944816580404, 4638),\n",
       " (0.0009336295661988143, 7370),\n",
       " (0.0009216265771189666, 4484),\n",
       " (0.0009142725305404148, 4420),\n",
       " (0.0009120807063992532, 2942),\n",
       " (0.0009062256510492705, 74),\n",
       " (0.0008900742506810209, 5218),\n",
       " (0.0008787196829778962, 7146),\n",
       " (0.0008740421140243623, 4469),\n",
       " (0.000871383286017146, 724),\n",
       " (0.000836241693140646, 522),\n",
       " (0.0008227828680914251, 105),\n",
       " (0.0008155542852888359, 1190),\n",
       " (0.0008150772867761924, 5405),\n",
       " (0.000811825544036961, 985),\n",
       " (0.0008002386589806485, 1104),\n",
       " (0.0007982889991320443, 474),\n",
       " (0.0007975359869153858, 5839),\n",
       " (0.000791669707766968, 7095),\n",
       " (0.0007904840487080189, 5282),\n",
       " (0.0007746715756527786, 7924),\n",
       " (0.0007718402121618471, 3179),\n",
       " (0.0007665583893332057, 364),\n",
       " (0.0007634183717083884, 3610),\n",
       " (0.0007542706759003783, 311),\n",
       " (0.0007358482686497311, 6706),\n",
       " (0.0007274059052945773, 4293),\n",
       " (0.0007133151227664946, 8075),\n",
       " (0.0007043180084852285, 7707),\n",
       " (0.0006994995252185746, 7184),\n",
       " (0.0006993346728933452, 4866),\n",
       " (0.0006987736142013526, 2514),\n",
       " (0.0006843583024026259, 590),\n",
       " (0.0006808606919840591, 2419),\n",
       " (0.0006675740843631074, 7620),\n",
       " (0.000660164950879739, 6032),\n",
       " (0.0006359307859696745, 7814),\n",
       " (0.0006253305207602621, 6425),\n",
       " (0.0006220971987131158, 3080),\n",
       " (0.0006199012168534451, 575),\n",
       " (0.0006173675710846513, 3373),\n",
       " (0.0006154383123747105, 434),\n",
       " (0.000611068137817922, 3149),\n",
       " (0.0006064489870655825, 6579),\n",
       " (0.0006042478128390882, 7008),\n",
       " (0.0006023317334201732, 5148),\n",
       " (0.0006020209993303315, 2841),\n",
       " (0.000596990302285817, 1057),\n",
       " (0.0005935664580191921, 4438),\n",
       " (0.0005897349260899595, 4937),\n",
       " (0.000588640639548662, 5142),\n",
       " (0.0005866793986571394, 5496),\n",
       " (0.0005850493462976619, 822),\n",
       " (0.0005839666766052343, 6666),\n",
       " (0.0005748561940778252, 999),\n",
       " (0.0005718554472727447, 7581),\n",
       " (0.0005548769385371583, 705),\n",
       " (0.0005540847368234035, 6028),\n",
       " (0.0005502758777826085, 310),\n",
       " (0.0005495906816901111, 7479),\n",
       " (0.0005486027055757311, 5116),\n",
       " (0.0005475125631352684, 6241),\n",
       " (0.0005234337552958587, 6242),\n",
       " (0.0005181817227492076, 7965),\n",
       " (0.0005130103555183659, 2766),\n",
       " (0.0005128228809790527, 1105),\n",
       " (0.0005069576459226043, 5952),\n",
       " (0.00050690696157069, 4663),\n",
       " (0.0005065004182402381, 2361),\n",
       " (0.0005044767675538335, 6545),\n",
       " (0.000501415883541808, 4020),\n",
       " (0.0005014029201099983, 7177),\n",
       " (0.0004949798990479677, 5570),\n",
       " (0.000493223571940735, 3637),\n",
       " (0.000492336136328202, 2397),\n",
       " (0.0004919521527322626, 7672),\n",
       " (0.00048566848351687576, 3224),\n",
       " (0.0004855577446577111, 6703),\n",
       " (0.0004768097167184697, 3451),\n",
       " (0.0004766019245906701, 335),\n",
       " (0.0004744958066375889, 2173),\n",
       " (0.00047230197674311353, 554),\n",
       " (0.00046790902731082593, 596),\n",
       " (0.0004635571653544418, 7114),\n",
       " (0.0004621003871449137, 7919),\n",
       " (0.000461642392207308, 3841),\n",
       " (0.0004599330044674363, 6286),\n",
       " (0.0004557019271890935, 869),\n",
       " (0.0004520779718574142, 6422),\n",
       " (0.00044770910687377986, 5245),\n",
       " (0.00044746030850884743, 3225),\n",
       " (0.00044568798150597806, 1056),\n",
       " (0.00044529210786354895, 8015),\n",
       " (0.0004427703982906673, 6997),\n",
       " (0.00044155384316766093, 6837),\n",
       " (0.00044149265908741657, 7014),\n",
       " (0.00043678160526086575, 6953),\n",
       " (0.0004343456326052204, 6693),\n",
       " (0.0004307721897085082, 7168),\n",
       " (0.0004282248232480913, 6087),\n",
       " (0.000418905546724374, 4400),\n",
       " (0.00041770560337532126, 3515),\n",
       " (0.0004164834620719331, 5532),\n",
       " (0.0004134564910015135, 6612),\n",
       " (0.0004084934017226717, 3278),\n",
       " (0.00040544979870564253, 3410),\n",
       " (0.0003976409325774094, 815),\n",
       " (0.00039319260225043534, 5904),\n",
       " (0.0003931147961369368, 1882),\n",
       " (0.00039304832056631986, 2120),\n",
       " (0.00039237033068569694, 3350),\n",
       " (0.00039105128638758894, 2788),\n",
       " (0.00038836644540706005, 5506),\n",
       " (0.00038559158547570976, 6805),\n",
       " (0.0003850939154508741, 2008),\n",
       " (0.0003819481951107519, 3710),\n",
       " (0.000381642679943388, 2459),\n",
       " (0.00038076145575829246, 2848),\n",
       " (0.0003802594615218629, 344),\n",
       " (0.00037641406845947674, 4380),\n",
       " (0.00037108567033478405, 5258),\n",
       " (0.00037084021754362753, 4535),\n",
       " (0.00036992967230616047, 3740),\n",
       " (0.0003698502284315106, 7643),\n",
       " (0.00036978323611952415, 280),\n",
       " (0.00036939022601264695, 146),\n",
       " (0.0003669160593006354, 5141),\n",
       " (0.00036529289340165173, 4995),\n",
       " (0.00036470388091432604, 6611),\n",
       " (0.0003631356159614412, 2528),\n",
       " (0.00036274784783170427, 217),\n",
       " (0.00035896751210247736, 93),\n",
       " (0.00035846212616984945, 3939),\n",
       " (0.000357543803391056, 852),\n",
       " (0.0003574061565346294, 3207),\n",
       " (0.0003546344102223459, 7805),\n",
       " (0.0003532979582559215, 1900),\n",
       " (0.00035323443863432744, 6442),\n",
       " (0.0003529811229810458, 5172),\n",
       " (0.00034943619516569204, 5211),\n",
       " (0.00034922072147417925, 2608),\n",
       " (0.0003481358094090278, 4801),\n",
       " (0.0003476112901831347, 3608),\n",
       " (0.0003460350669764117, 3387),\n",
       " (0.00034469544752436875, 161),\n",
       " (0.000343860123503603, 669),\n",
       " (0.00034278136620722997, 0),\n",
       " (0.00034172051549559033, 7667),\n",
       " (0.0003406785655161426, 4063),\n",
       " (0.00033939680082957296, 3162),\n",
       " (0.00033651465670012494, 1158),\n",
       " (0.0003347198681832863, 2286),\n",
       " (0.0003344885457744766, 3283),\n",
       " (0.0003337319776843773, 7945),\n",
       " (0.0003334410846451838, 4906),\n",
       " (0.0003290069655387191, 1005),\n",
       " (0.00032872816613512045, 5081),\n",
       " (0.00032689580510961, 4419),\n",
       " (0.00032473420550990686, 7034),\n",
       " (0.0003237142930532674, 4342),\n",
       " (0.00032309153961775165, 7936),\n",
       " (0.00032292383391080236, 878),\n",
       " (0.00032131171968803574, 888),\n",
       " (0.00031869886880457105, 1656),\n",
       " (0.0003155005770167561, 1250),\n",
       " (0.0003145017931791902, 318),\n",
       " (0.00031368531130785126, 4800),\n",
       " (0.0003126802082853368, 2440),\n",
       " (0.0003079780377763291, 7352),\n",
       " (0.00030752945915406754, 5705),\n",
       " (0.0003054859519509734, 7443),\n",
       " (0.00030498009255163893, 4776),\n",
       " (0.00030415714546251925, 6650),\n",
       " (0.0003035574014357522, 6088),\n",
       " (0.000299996105765001, 2176),\n",
       " (0.00029927150554594787, 4421),\n",
       " (0.00029906137021242695, 3569),\n",
       " (0.0002987968961583942, 4829),\n",
       " (0.00029794777185617214, 3989),\n",
       " (0.0002973984808679709, 1390),\n",
       " (0.00029676808418361905, 8003),\n",
       " (0.00029373892106376973, 3022),\n",
       " (0.0002935664267935887, 7584),\n",
       " (0.000293501355131434, 152),\n",
       " (0.0002934327462228845, 289),\n",
       " (0.00029155907632952115, 2551),\n",
       " (0.0002899752354940145, 879),\n",
       " (0.0002876646903370895, 473),\n",
       " (0.00028716039195315435, 6136),\n",
       " (0.000286604958870442, 6812),\n",
       " (0.000286577518888016, 2547),\n",
       " (0.00028381279078625954, 420),\n",
       " (0.0002813925385941486, 4795),\n",
       " (0.000281250371468985, 350),\n",
       " (0.00028018912593727333, 5352),\n",
       " (0.0002771458233898909, 156),\n",
       " (0.0002768338948732115, 525),\n",
       " (0.00027420281768005087, 941),\n",
       " (0.0002725074992123518, 5918),\n",
       " (0.00027106401046707795, 7812),\n",
       " (0.00026773527746533495, 2104),\n",
       " (0.00026330093485918594, 835),\n",
       " (0.0002616617984266312, 2026),\n",
       " (0.0002616428776547933, 7968),\n",
       " (0.0002614409299656472, 6809),\n",
       " (0.00026041428702080967, 224),\n",
       " (0.00026027490824408027, 4957),\n",
       " (0.00025999585265981976, 1859),\n",
       " (0.00025812343982250347, 370),\n",
       " (0.0002560815107398977, 1155),\n",
       " (0.0002555194661131204, 4027),\n",
       " (0.0002538563056050107, 6511),\n",
       " (0.00025312867383775256, 390),\n",
       " (0.00025013846191944216, 1815),\n",
       " (0.0002485006135947498, 482),\n",
       " (0.0002478130248459702, 381),\n",
       " (0.0002474264161731112, 8006),\n",
       " (0.0002464543818192128, 17),\n",
       " (0.00024552662893871863, 3552),\n",
       " (0.00024245911280434748, 3801),\n",
       " (0.00024228950228650077, 4536),\n",
       " (0.0002418183232501616, 2574),\n",
       " (0.0002411590751305268, 5030),\n",
       " (0.0002409454185295835, 79),\n",
       " (0.00024070100898849087, 976),\n",
       " (0.00024022324143849742, 725),\n",
       " (0.0002392512365877108, 7205),\n",
       " (0.00023897835207401023, 2363),\n",
       " (0.00023749533387611282, 2241),\n",
       " (0.00023742162385571291, 6873),\n",
       " (0.00023621473280849417, 902),\n",
       " (0.0002356584636293941, 7603),\n",
       " (0.0002328908634723706, 7319),\n",
       " (0.00023274889607556183, 3605),\n",
       " (0.0002320449093563752, 314),\n",
       " (0.00023064766336681762, 6498),\n",
       " (0.00022974704725453837, 4412),\n",
       " (0.00022953356609181866, 5007),\n",
       " (0.0002285541879736023, 3954),\n",
       " (0.0002278340349176643, 3884),\n",
       " (0.0002276652066741639, 3585),\n",
       " (0.0002266370183537747, 1290),\n",
       " (0.00022639684458686463, 2783),\n",
       " (0.00022499172396081728, 5285),\n",
       " (0.00022427956846259484, 1191),\n",
       " (0.0002241338350188421, 6277),\n",
       " (0.0002240470574211801, 6092),\n",
       " (0.00022394441997158578, 1271),\n",
       " (0.00022347438462103647, 2823),\n",
       " (0.0002232075775609011, 561),\n",
       " (0.00022302385700223416, 1934),\n",
       " (0.00022254517394286363, 2674),\n",
       " (0.00022142954097073057, 7606),\n",
       " (0.0002208997755808925, 6492),\n",
       " (0.0002207669547480356, 7206),\n",
       " (0.0002199898380522933, 2155),\n",
       " (0.0002199091935240976, 8101),\n",
       " (0.00021960650894785834, 5815),\n",
       " (0.00021809286946432302, 2126),\n",
       " (0.000217946554626273, 2148),\n",
       " (0.00021689345435448596, 6445),\n",
       " (0.000216180414397718, 3669),\n",
       " (0.0002149052504189262, 4450),\n",
       " (0.00021425627898771015, 130),\n",
       " (0.0002138399499809724, 471),\n",
       " (0.0002134881753626805, 562),\n",
       " (0.00021306490311823764, 6868),\n",
       " (0.00021220712056611946, 2381),\n",
       " (0.00021164275677200097, 2325),\n",
       " (0.0002107926593721367, 4446),\n",
       " (0.00021071968818707896, 1743),\n",
       " (0.0002094203798917639, 5982),\n",
       " (0.00020869365971581, 3493),\n",
       " (0.0002084640379507224, 1776),\n",
       " (0.0002054011598382991, 403),\n",
       " (0.0002053018021696552, 1149),\n",
       " (0.00020504573206363397, 4144),\n",
       " (0.0002048283192968835, 4434),\n",
       " (0.00020339911805289628, 4369),\n",
       " (0.0002020960141921188, 7253),\n",
       " (0.0002020179319452901, 7717),\n",
       " (0.00020154805873093693, 2943),\n",
       " (0.00020012172051286743, 90),\n",
       " (0.00019901432952586166, 7348),\n",
       " (0.00019869689590513079, 2689),\n",
       " (0.0001976834244783041, 7851),\n",
       " (0.0001956465275687642, 6811),\n",
       " (0.00019551803521427935, 2859),\n",
       " (0.0001945812463446205, 3607),\n",
       " (0.0001943065791922326, 1789),\n",
       " (0.00019412257918795402, 1847),\n",
       " (0.00019356747868446233, 1220),\n",
       " (0.00019156214169103292, 1028),\n",
       " (0.0001913042567635803, 7430),\n",
       " (0.00018987368349364565, 4414),\n",
       " (0.00018929343748352807, 4584),\n",
       " (0.00018891936217474762, 1412),\n",
       " (0.00018796341698683723, 6094),\n",
       " (0.00018783837661004595, 4947),\n",
       " (0.0001874114208746622, 4284),\n",
       " (0.0001858070354983886, 7193),\n",
       " (0.00018575589986885344, 5015),\n",
       " (0.00018515933655687843, 6210),\n",
       " (0.00018355182436962506, 5435),\n",
       " (0.0001834862511376815, 4339),\n",
       " (0.00018320486855849345, 3146),\n",
       " (0.00018234207667193372, 3993),\n",
       " (0.00018192355962396087, 757),\n",
       " (0.0001807152134525284, 6850),\n",
       " (0.00018017116105710287, 6447),\n",
       " (0.0001801371124293787, 1),\n",
       " (0.00017981034890377238, 3789),\n",
       " (0.00017977117922550665, 2908),\n",
       " (0.00017916424542130258, 1006),\n",
       " (0.0001788280234405371, 2837),\n",
       " (0.00017873608035750933, 158),\n",
       " (0.000178388666847434, 4930),\n",
       " (0.00017778203361004696, 4287),\n",
       " (0.0001773066945403811, 4427),\n",
       " (0.00017696554801276772, 458),\n",
       " (0.00017649625076618196, 7949),\n",
       " (0.0001764229120711776, 880),\n",
       " (0.00017478621796858832, 389),\n",
       " (0.000174500672063187, 7473),\n",
       " (0.00017406194569862042, 680),\n",
       " (0.00017345982378275647, 7908),\n",
       " (0.00017031519144152372, 830),\n",
       " (0.00017017641276124404, 6842),\n",
       " (0.0001691053400683734, 1236),\n",
       " (0.00016881461726771753, 104),\n",
       " (0.0001686653537343736, 7976),\n",
       " (0.0001686389818769187, 1372),\n",
       " (0.00016632671975780355, 1207),\n",
       " (0.00016546075685383937, 65),\n",
       " (0.00016533359683564252, 5401),\n",
       " (0.00016464466582916934, 3733),\n",
       " (0.0001630406106988295, 6754),\n",
       " (0.00016236269761909033, 3791),\n",
       " (0.00016234546200223678, 1095),\n",
       " (0.0001604672325193145, 3303),\n",
       " (0.0001602752589652182, 527),\n",
       " (0.00016009409367147526, 5823),\n",
       " (0.0001600545336764454, 5394),\n",
       " (0.00015940156981414196, 2665),\n",
       " (0.00015933420998151461, 3272),\n",
       " (0.00015883104282252218, 6935),\n",
       " (0.00015881376894848827, 3404),\n",
       " (0.00015764594500709464, 5828),\n",
       " (0.0001571232966883926, 3324),\n",
       " (0.00015708981297001738, 7695),\n",
       " (0.000157083048998158, 3009),\n",
       " (0.000157030125435786, 6537),\n",
       " (0.00015701505967476625, 4504),\n",
       " (0.00015664447603418616, 2941),\n",
       " (0.00015583729795927187, 4830),\n",
       " (0.00015581050745250866, 4978),\n",
       " (0.00015528761024636474, 870),\n",
       " (0.00015435386400324493, 7895),\n",
       " (0.00015318131484447274, 7836),\n",
       " (0.00015272216384248807, 7495),\n",
       " (0.00015240707805186284, 1799),\n",
       " (0.0001521491136214736, 3697),\n",
       " (0.0001517518405125207, 633),\n",
       " (0.00015162889156227364, 4576),\n",
       " (0.000151471578743967, 1844),\n",
       " (0.00015074205996164658, 7130),\n",
       " (0.00014908296403822146, 2563),\n",
       " (0.00014884321302857038, 5533),\n",
       " (0.00014838136626553846, 4384),\n",
       " (0.00014807234382719478, 162),\n",
       " (0.00014675947059788864, 3158),\n",
       " (0.00014658225777094099, 1049),\n",
       " (0.00014486224846472714, 3531),\n",
       " (0.00014324580351279643, 2016),\n",
       " (0.00014319805181353155, 3854),\n",
       " (0.00014266892050080805, 4388),\n",
       " (0.00014246414917675295, 2248),\n",
       " (0.00014237625479843698, 4261),\n",
       " (0.00014190779889544983, 13),\n",
       " (0.000141900075901368, 3385),\n",
       " (0.0001408998150583998, 3275),\n",
       " (0.0001406661479740362, 1126),\n",
       " (0.00014053067248810417, 2965),\n",
       " (0.00013973749980541745, 2070),\n",
       " (0.0001390191799663909, 2210),\n",
       " (0.0001388553594838552, 3015),\n",
       " (0.00013866953988821192, 2000),\n",
       " (0.0001386601669565345, 584),\n",
       " (0.0001386461997386078, 1131),\n",
       " (0.00013823898854968647, 905),\n",
       " (0.000138194229173642, 7837),\n",
       " (0.00013763626269210584, 2313),\n",
       " (0.00013726653368500635, 1564),\n",
       " (0.00013695430692699356, 5919),\n",
       " (0.0001365604169366333, 2620),\n",
       " (0.00013622285170146353, 6184),\n",
       " (0.00013590963252582108, 1623),\n",
       " (0.0001358819923251939, 6289),\n",
       " (0.00013567855663902984, 2872),\n",
       " (0.0001343412128923381, 4640),\n",
       " (0.00013354782293073262, 572),\n",
       " (0.0001331478536263137, 3542),\n",
       " (0.00013314740740579607, 1148),\n",
       " (0.00013259218929652535, 1384),\n",
       " (0.00013193968858984565, 7047),\n",
       " (0.00013178921835668925, 917),\n",
       " (0.00013168938784478476, 842),\n",
       " (0.00013103343907784333, 831),\n",
       " (0.00012971918951215947, 6271),\n",
       " (0.00012953601409789738, 1275),\n",
       " (0.00012922961781493194, 4570),\n",
       " (0.00012895563607446384, 1861),\n",
       " (0.0001288991867242784, 201),\n",
       " (0.00012872440502627772, 446),\n",
       " (0.00012860782564519044, 5862),\n",
       " (0.00012850715180162423, 7144),\n",
       " (0.00012837504492309749, 4013),\n",
       " (0.00012801842389109132, 5840),\n",
       " (0.00012693032643058835, 174),\n",
       " (0.00012636930604902196, 520),\n",
       " (0.00012633509136546106, 825),\n",
       " (0.0001259220242982732, 4635),\n",
       " (0.00012574392808069638, 2935),\n",
       " (0.000125684238722002, 7569),\n",
       " (0.00012551220450547802, 4057),\n",
       " (0.00012533449027645035, 811),\n",
       " (0.0001251821493776386, 6942),\n",
       " (0.0001248670929237444, 1820),\n",
       " (0.00012463766047312696, 3876),\n",
       " (0.00012436606588137853, 2940),\n",
       " (0.00012390669146119124, 7093),\n",
       " (0.00012329499481267197, 7465),\n",
       " (0.0001226847892752221, 3488),\n",
       " (0.00012226376516805428, 4356),\n",
       " (0.00012164014770432491, 606),\n",
       " (0.000121392236473971, 3327),\n",
       " (0.00012110449821569244, 1070),\n",
       " (0.00012097479149920198, 1593),\n",
       " (0.00012095234123311985, 6564),\n",
       " (0.00012065933835410852, 3878),\n",
       " (0.00012051643852551571, 142),\n",
       " (0.00012050209871194636, 4739),\n",
       " (0.00012033729222059135, 6594),\n",
       " (0.00011997854230152783, 3055),\n",
       " (0.00011992775546234022, 5852),\n",
       " (0.00011961587795066494, 728),\n",
       " (0.0001194851049477609, 6653),\n",
       " (0.00011862801394325809, 356),\n",
       " (0.0001182875248902538, 789),\n",
       " (0.00011803288526970536, 7225),\n",
       " (0.00011798694585656835, 5810),\n",
       " (0.00011786918077382399, 2586),\n",
       " (0.00011745640110835345, 6176),\n",
       " (0.00011739916102457732, 4542),\n",
       " (0.0001172704752761219, 3720),\n",
       " (0.00011680807772331734, 5978),\n",
       " (0.00011652886463540454, 5726),\n",
       " (0.00011630454637370168, 5507),\n",
       " (0.00011621642254011573, 6040),\n",
       " (0.00011442543099700226, 3860),\n",
       " (0.00011414643258584615, 4367),\n",
       " (0.00011390607303207614, 4386),\n",
       " (0.00011388957459309529, 6410),\n",
       " (0.00011382342175691895, 3662),\n",
       " (0.00011350343623644896, 6012),\n",
       " (0.00011279892223560205, 2003),\n",
       " (0.00011262834820289021, 1125),\n",
       " (0.00011155899571187133, 3691),\n",
       " (0.00011154694485643499, 2389),\n",
       " (0.00011141756241667535, 4177),\n",
       " (0.00011120561082937432, 4324),\n",
       " (0.00011114952219990107, 1816),\n",
       " (0.00011086046681591499, 8018),\n",
       " (0.00011023385073875884, 5414),\n",
       " (0.0001101701603501201, 765),\n",
       " (0.00010902526027942432, 3003),\n",
       " (0.00010887770945363494, 959),\n",
       " (0.0001087882443690276, 341),\n",
       " (0.00010833943963346136, 7531),\n",
       " (0.00010800361169084964, 6738),\n",
       " (0.00010764998083646526, 1077),\n",
       " (0.0001073606279899917, 1433),\n",
       " (0.00010680625572853323, 5469),\n",
       " (0.00010672428562417091, 7416),\n",
       " (0.0001065386914881027, 1428),\n",
       " (0.0001060129553232921, 3879),\n",
       " (0.00010600368616333284, 3955),\n",
       " (0.00010597815434201952, 6609),\n",
       " (0.00010502112829325172, 139),\n",
       " (0.00010487326928607079, 2892),\n",
       " (0.00010475892999107237, 6306),\n",
       " (0.00010448732554426008, 1000),\n",
       " (0.00010440872154377375, 7051),\n",
       " (0.00010424053647755494, 5351),\n",
       " (0.00010400088841592909, 5559),\n",
       " (0.00010359976398060161, 7277),\n",
       " (0.00010314235761684682, 7922),\n",
       " (0.00010304722398641847, 1274),\n",
       " (0.00010292001463218869, 7023),\n",
       " (0.0001027365134766521, 1300),\n",
       " (0.00010251090345487473, 5500),\n",
       " (0.00010247024532689813, 3572),\n",
       " (0.00010134026443425802, 735),\n",
       " (0.00010079584398614592, 1956),\n",
       " (0.00010053535552002147, 2778),\n",
       " (0.00010040133430335063, 7846),\n",
       " (0.00010022629568259919, 699),\n",
       " (0.00010017118494158482, 1419),\n",
       " (9.974048223932133e-05, 6142),\n",
       " (9.97192989253521e-05, 641),\n",
       " (9.955124765498866e-05, 4813),\n",
       " (9.859262621886518e-05, 3630),\n",
       " (9.853407776939356e-05, 117),\n",
       " (9.847715941161779e-05, 4594),\n",
       " (9.83624886375055e-05, 3297),\n",
       " (9.82741959407284e-05, 7315),\n",
       " (9.695276703427922e-05, 4075),\n",
       " (9.690968680963812e-05, 8062),\n",
       " (9.674733646722755e-05, 6141),\n",
       " (9.665724051762751e-05, 4351),\n",
       " (9.611872435215366e-05, 7500),\n",
       " (9.609687730022653e-05, 2433),\n",
       " (9.604490837437162e-05, 2123),\n",
       " (9.60329269101482e-05, 2640),\n",
       " (9.598208556123787e-05, 5010),\n",
       " (9.573071142451421e-05, 292),\n",
       " (9.550283112128618e-05, 1416),\n",
       " (9.530811529531587e-05, 5),\n",
       " (9.517614838059504e-05, 7334),\n",
       " (9.514439759257558e-05, 919),\n",
       " (9.459392472179433e-05, 914),\n",
       " (9.452852952869398e-05, 5495),\n",
       " (9.428167515073152e-05, 2125),\n",
       " (9.383565373695057e-05, 7798),\n",
       " (9.363955572685406e-05, 4862),\n",
       " (9.211348896516265e-05, 1991),\n",
       " (9.201526974185198e-05, 47),\n",
       " (9.18878782316062e-05, 1938),\n",
       " (9.183204547210396e-05, 4310),\n",
       " (9.177878275809477e-05, 891),\n",
       " (9.164107998164018e-05, 4591),\n",
       " (9.14767691340042e-05, 5036),\n",
       " (9.14097203591132e-05, 1067),\n",
       " (9.112597173805182e-05, 589),\n",
       " (9.05736867633551e-05, 3152),\n",
       " (9.054850282057846e-05, 4232),\n",
       " (9.039055086453443e-05, 6681),\n",
       " (9.006042556934385e-05, 4648),\n",
       " (9.002000745836342e-05, 7271),\n",
       " (8.989193183537053e-05, 418),\n",
       " (8.905192665027725e-05, 8033),\n",
       " (8.899869196803495e-05, 4468),\n",
       " (8.875728055928149e-05, 5898),\n",
       " (8.810106443806402e-05, 6826),\n",
       " (8.801265799909026e-05, 4630),\n",
       " (8.794201380528956e-05, 3804),\n",
       " (8.763663911426876e-05, 7560),\n",
       " (8.754762453402731e-05, 4624),\n",
       " (8.74671432554602e-05, 1108),\n",
       " (8.704447547279425e-05, 5537),\n",
       " (8.677492882995302e-05, 3831),\n",
       " (8.67044594595673e-05, 2244),\n",
       " (8.665748713160912e-05, 5302),\n",
       " (8.645751731357439e-05, 8031),\n",
       " (8.62625510175843e-05, 5955),\n",
       " (8.610416143589964e-05, 5597),\n",
       " (8.605787224304951e-05, 296),\n",
       " (8.602840294608635e-05, 3744),\n",
       " (8.594748210767789e-05, 28),\n",
       " (8.57109119233917e-05, 6345),\n",
       " (8.558443588324844e-05, 5731),\n",
       " (8.552525329949674e-05, 3692),\n",
       " (8.51991099900002e-05, 6853),\n",
       " (8.496398380903251e-05, 5044),\n",
       " (8.4082796904033e-05, 4803),\n",
       " (8.386573461074018e-05, 7163),\n",
       " (8.300534009661985e-05, 5927),\n",
       " (8.293381174523498e-05, 3536),\n",
       " (8.263324843899796e-05, 7553),\n",
       " (8.184232827674385e-05, 4790),\n",
       " (8.170010267602211e-05, 7656),\n",
       " (8.133340229942918e-05, 1313),\n",
       " (8.102401838026624e-05, 932),\n",
       " (8.057651249106216e-05, 2656),\n",
       " (8.025862018801403e-05, 3199),\n",
       " (7.999513983111049e-05, 7567),\n",
       " (7.942608860518086e-05, 769),\n",
       " (7.936770556790932e-05, 7393),\n",
       " (7.930784689636516e-05, 5084),\n",
       " (7.93007435712352e-05, 6602),\n",
       " (7.892087063636367e-05, 3497),\n",
       " (7.853025545357788e-05, 159),\n",
       " (7.835585528409031e-05, 824),\n",
       " (7.79805271096861e-05, 1090),\n",
       " (7.793492785212576e-05, 5746),\n",
       " (7.775102080153245e-05, 236),\n",
       " (7.752446807529507e-05, 257),\n",
       " (7.714684626532004e-05, 5620),\n",
       " (7.659413332481213e-05, 3758),\n",
       " (7.652526466100138e-05, 2786),\n",
       " (7.622905863153851e-05, 7842),\n",
       " (7.602253576420038e-05, 4965),\n",
       " (7.568426582710702e-05, 1184),\n",
       " (7.490733933335599e-05, 1402),\n",
       " (7.478766776352383e-05, 6781),\n",
       " (7.461462538992877e-05, 3579),\n",
       " (7.400273819068124e-05, 777),\n",
       " (7.382926291498593e-05, 6522),\n",
       " (7.349435976706787e-05, 6397),\n",
       " (7.329343174771811e-05, 4066),\n",
       " (7.318703759230464e-05, 4105),\n",
       " (7.318203690706359e-05, 5648),\n",
       " (7.273741388987908e-05, 5283),\n",
       " (7.248599503121211e-05, 1047),\n",
       " (7.228521181975006e-05, 1822),\n",
       " (7.169424275491844e-05, 6731),\n",
       " (7.168822859509051e-05, 6975),\n",
       " (7.155551745155337e-05, 1011),\n",
       " (7.127112727893785e-05, 3507),\n",
       " (7.124690975868328e-05, 7336),\n",
       " (7.120847842449293e-05, 5493),\n",
       " (7.072886589332197e-05, 4280),\n",
       " (7.063978029832289e-05, 2367),\n",
       " (7.059156318831509e-05, 2195),\n",
       " (7.044636083850449e-05, 2540),\n",
       " (7.024531171284125e-05, 1950),\n",
       " (6.972967427111131e-05, 1642),\n",
       " (6.972066158802943e-05, 1709),\n",
       " (6.941654424753343e-05, 5573),\n",
       " (6.931901293834021e-05, 2916),\n",
       " (6.905222621208707e-05, 8079),\n",
       " (6.835727842116433e-05, 6788),\n",
       " (6.835608303345561e-05, 2288),\n",
       " (6.814671011055607e-05, 729),\n",
       " (6.810257302814485e-05, 4267),\n",
       " (6.788133760667335e-05, 5350),\n",
       " (6.722363063141119e-05, 4493),\n",
       " (6.694561039552577e-05, 7898),\n",
       " (6.690673200260072e-05, 5576),\n",
       " (6.660569395101355e-05, 761),\n",
       " (6.645846513168754e-05, 3498),\n",
       " (6.594932349089499e-05, 4931),\n",
       " (6.578797573292798e-05, 755),\n",
       " (6.556538181387532e-05, 1763),\n",
       " (6.516217351217175e-05, 2998),\n",
       " (6.495902690372389e-05, 5107),\n",
       " (6.44606115807584e-05, 2612),\n",
       " (6.419073120859936e-05, 4478),\n",
       " (6.417510085818361e-05, 5090),\n",
       " (6.380012652642002e-05, 2869),\n",
       " (6.369007103270935e-05, 64),\n",
       " (6.354944219598198e-05, 2237),\n",
       " (6.345035069347179e-05, 115),\n",
       " (6.300544370369082e-05, 37),\n",
       " (6.285585205946092e-05, 3306),\n",
       " (6.231091429122013e-05, 3571),\n",
       " (6.21321740367002e-05, 1632),\n",
       " (6.206825094257988e-05, 1907),\n",
       " (6.186641936594077e-05, 39),\n",
       " (6.161421037187768e-05, 7678),\n",
       " (6.130866819917739e-05, 3214),\n",
       " (6.130523790175881e-05, 2226),\n",
       " (6.114150149936438e-05, 6599),\n",
       " (6.102895634298203e-05, 4816),\n",
       " (6.098179321844222e-05, 343),\n",
       " (6.097992572990851e-05, 7975),\n",
       " (6.092843489301799e-05, 6418),\n",
       " (6.051903778337088e-05, 143),\n",
       " (6.04174419431457e-05, 3187),\n",
       " (6.038253018468296e-05, 3836),\n",
       " (6.0310648107830495e-05, 4831),\n",
       " (6.025945857719225e-05, 1339),\n",
       " (6.019396514101051e-05, 5831),\n",
       " (6.011431918741075e-05, 1531),\n",
       " (5.993392370280912e-05, 2021),\n",
       " (5.981817023034682e-05, 3195),\n",
       " (5.9613041164108194e-05, 278),\n",
       " (5.931619543900957e-05, 2497),\n",
       " (5.9290801269171764e-05, 3322),\n",
       " (5.872586946124958e-05, 4688),\n",
       " (5.8694980442046016e-05, 3714),\n",
       " (5.863775349200003e-05, 2198),\n",
       " (5.856654111836105e-05, 2909),\n",
       " (5.84672537647056e-05, 931),\n",
       " (5.8316436834727254e-05, 7655),\n",
       " (5.825593979317972e-05, 4321),\n",
       " (5.823233673119136e-05, 6025),\n",
       " (5.802719534834797e-05, 5980),\n",
       " (5.800085872716006e-05, 2829),\n",
       " (5.781837231571259e-05, 6915),\n",
       " (5.768545780610119e-05, 1905),\n",
       " (5.767535665432362e-05, 95),\n",
       " (5.763085406431258e-05, 447),\n",
       " (5.754165450372911e-05, 5354),\n",
       " (5.731763021202659e-05, 5272),\n",
       " (5.7136804702159836e-05, 7401),\n",
       " (5.7094595309548184e-05, 5903),\n",
       " (5.707041480562848e-05, 7219),\n",
       " (5.696694099310075e-05, 5571),\n",
       " (5.6899142029566844e-05, 882),\n",
       " (5.649002401032966e-05, 5801),\n",
       " (5.641542198618912e-05, 2529),\n",
       " (5.6370932462834944e-05, 826),\n",
       " (5.596026675504849e-05, 5784),\n",
       " (5.5727429035138075e-05, 4322),\n",
       " (5.557773395863403e-05, 3850),\n",
       " (5.492533566792639e-05, 113),\n",
       " (5.475863812656802e-05, 3905),\n",
       " (5.4628618642380805e-05, 1071),\n",
       " (5.456155218449592e-05, 585),\n",
       " (5.45609890919163e-05, 6189),\n",
       " (5.4452736540320284e-05, 7911),\n",
       " (5.434364908176757e-05, 4039),\n",
       " (5.431174790826812e-05, 1025),\n",
       " (5.417464110639902e-05, 6830),\n",
       " (5.408358179090344e-05, 3437),\n",
       " (5.396268118760074e-05, 903),\n",
       " (5.377654125803857e-05, 5220),\n",
       " (5.370853495034605e-05, 4714),\n",
       " (5.360941875988541e-05, 5002),\n",
       " (5.339336952484776e-05, 6729),\n",
       " (5.334515270187498e-05, 2562),\n",
       " (5.331610127386069e-05, 746),\n",
       " (5.3313309569209855e-05, 6109),\n",
       " (5.328802490511868e-05, 4935),\n",
       " (5.325171814826422e-05, 430),\n",
       " (5.324808546713654e-05, 7600),\n",
       " (5.305532333353112e-05, 4392),\n",
       " (5.3041826325016864e-05, 352),\n",
       " (5.297983522374216e-05, 613),\n",
       " (5.292957747710834e-05, 4636),\n",
       " (5.2727821781338574e-05, 7546),\n",
       " (5.2241350469739876e-05, 6795),\n",
       " (5.220888945986649e-05, 5868),\n",
       " (5.2197130081579665e-05, 6817),\n",
       " (5.216208673388949e-05, 120),\n",
       " (5.212973880973746e-05, 1455),\n",
       " (5.2052642781555876e-05, 4411),\n",
       " (5.1931583433003584e-05, 4827),\n",
       " (5.1768200709835867e-05, 6212),\n",
       " (5.1754743940802366e-05, 7042),\n",
       " (5.16778989670926e-05, 6994),\n",
       " (5.158158039036336e-05, 2966),\n",
       " (5.134258519074195e-05, 6343),\n",
       " (5.1248575249486894e-05, 5677),\n",
       " (5.099159165694808e-05, 3959),\n",
       " (5.098542868399579e-05, 5158),\n",
       " (5.078423800131099e-05, 5921),\n",
       " (5.067859853620286e-05, 8038),\n",
       " (5.0627978807460846e-05, 5224),\n",
       " (5.0401436841234526e-05, 160),\n",
       " (4.996905726121941e-05, 5305),\n",
       " (4.972237363406708e-05, 4002),\n",
       " (4.939621702663502e-05, 260),\n",
       " (4.930250806257765e-05, 7941),\n",
       " (4.927835685140571e-05, 4502),\n",
       " (4.9223796896203695e-05, 112),\n",
       " (4.9112846918673084e-05, 5636),\n",
       " (4.9103500030668286e-05, 7112),\n",
       " (4.9086176499847477e-05, 5995),\n",
       " (4.8965555323835666e-05, 2191),\n",
       " (4.8742650382456436e-05, 4289),\n",
       " (4.86803770697405e-05, 4883),\n",
       " (4.834669845359938e-05, 2401),\n",
       " (4.8049202308191365e-05, 4545),\n",
       " (4.799740557887638e-05, 3565),\n",
       " (4.787048709698881e-05, 5485),\n",
       " (4.777717336742123e-05, 192),\n",
       " (4.76222810882886e-05, 2168),\n",
       " (4.7558029942351706e-05, 2698),\n",
       " (4.753369834238103e-05, 3211),\n",
       " (4.750999945353261e-05, 7321),\n",
       " (4.7494146993002605e-05, 6221),\n",
       " (4.7390671645215815e-05, 5977),\n",
       " (4.72195750003075e-05, 2532),\n",
       " (4.7178695583410005e-05, 1586),\n",
       " (4.70836311719138e-05, 4410),\n",
       " (4.693740994661839e-05, 4532),\n",
       " (4.691676581290672e-05, 496),\n",
       " (4.6742292352202404e-05, 2374),\n",
       " (4.671368417011315e-05, 7742),\n",
       " (4.660772131295901e-05, 6483),\n",
       " (4.6488279035177064e-05, 3747),\n",
       " (4.647087820447945e-05, 2886),\n",
       " (4.6385177426639275e-05, 4868),\n",
       " (4.620410026911854e-05, 6855),\n",
       " (4.601875337585612e-05, 1418),\n",
       " (4.598313727874742e-05, 1362),\n",
       " (4.56724888213822e-05, 3712),\n",
       " (4.5475803388344255e-05, 6203),\n",
       " (4.540653740846492e-05, 2583),\n",
       " (4.537627487581581e-05, 7160),\n",
       " (4.5368209266092834e-05, 5942),\n",
       " (4.53153158718669e-05, 828),\n",
       " (4.506468301154835e-05, 4183),\n",
       " (4.4876299587215043e-05, 45),\n",
       " (4.4765814725300465e-05, 443),\n",
       " (4.475320593496181e-05, 6292),\n",
       " (4.4685192200006165e-05, 5068),\n",
       " (4.458812232667445e-05, 7101),\n",
       " (4.456866806026099e-05, 2622),\n",
       " (4.456866806026099e-05, 552),\n",
       " (4.450348288965697e-05, 7477),\n",
       " (4.427292287242503e-05, 1153),\n",
       " (4.413997019568575e-05, 547),\n",
       " (4.4062599670606384e-05, 5989),\n",
       " (4.375206495627877e-05, 2667),\n",
       " (4.364231342896211e-05, 6967),\n",
       " (4.358936055925371e-05, 4104),\n",
       " (4.350064404537004e-05, 855),\n",
       " (4.3397823150909575e-05, 688),\n",
       " (4.3236257798323796e-05, 3249),\n",
       " (4.321570830456641e-05, 2755),\n",
       " (4.3208692783509274e-05, 3368),\n",
       " (4.318636451715476e-05, 36),\n",
       " (4.318420363222658e-05, 2557),\n",
       " (4.3100846366615245e-05, 6252),\n",
       " (4.297374105383894e-05, 3510),\n",
       " (4.297374105383894e-05, 2805),\n",
       " (4.28451906531365e-05, 3096),\n",
       " (4.283133454359226e-05, 2776),\n",
       " (4.27938208057329e-05, 3399),\n",
       " (4.265875992110123e-05, 7917),\n",
       " (4.208507799878092e-05, 5858),\n",
       " (4.193500171758087e-05, 1429),\n",
       " (4.1814863984041194e-05, 2052),\n",
       " (4.167150647644988e-05, 6768),\n",
       " (4.153905424614406e-05, 5574),\n",
       " (4.141471385202306e-05, 7513),\n",
       " (4.111739568923206e-05, 3514),\n",
       " (4.1026102530158676e-05, 3038),\n",
       " (4.09819406679556e-05, 7987),\n",
       " (4.069837582185344e-05, 2344),\n",
       " (4.037417049599699e-05, 1819),\n",
       " (4.034602518891393e-05, 5411),\n",
       " (3.9943263240978115e-05, 1808),\n",
       " (3.956922891005739e-05, 4819),\n",
       " (3.942803189366403e-05, 3256),\n",
       " (3.9253587498747105e-05, 5758),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.525 / Accuracy: 0.933\n"
     ]
    }
   ],
   "source": [
    "print ('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                          round(recall, 3),\n",
    "                                                          round((y_pred==y_test).sum()/len(y_pred), 3)\n",
    "                                                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementing Grid Search Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf1 = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf1_model = rf1.fit(X_train, y_train)\n",
    "    y_pred = rf1_model.predict(X_test)\n",
    "    precision, recall, fscore, support= score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print ('Est.: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, \n",
    "        depth, \n",
    "        round(precision, 3),\n",
    "        round(recall, 3),\n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est.: 10 / Depth: 10 ---- Precision: 1.0 / Recall: 0.228 / Accuracy: 0.89\n",
      "Est.: 10 / Depth: 20 ---- Precision: 1.0 / Recall: 0.589 / Accuracy: 0.942\n",
      "Est.: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.671 / Accuracy: 0.953\n",
      "Est.: 10 / Depth: None ---- Precision: 0.992 / Recall: 0.791 / Accuracy: 0.969\n",
      "Est.: 50 / Depth: 10 ---- Precision: 1.0 / Recall: 0.209 / Accuracy: 0.888\n",
      "Est.: 50 / Depth: 20 ---- Precision: 1.0 / Recall: 0.551 / Accuracy: 0.936\n",
      "Est.: 50 / Depth: 30 ---- Precision: 1.0 / Recall: 0.658 / Accuracy: 0.952\n",
      "Est.: 50 / Depth: None ---- Precision: 1.0 / Recall: 0.772 / Accuracy: 0.968\n",
      "Est.: 100 / Depth: 10 ---- Precision: 1.0 / Recall: 0.215 / Accuracy: 0.889\n",
      "Est.: 100 / Depth: 20 ---- Precision: 1.0 / Recall: 0.532 / Accuracy: 0.934\n",
      "Est.: 100 / Depth: 30 ---- Precision: 1.0 / Recall: 0.671 / Accuracy: 0.953\n",
      "Est.: 100 / Depth: None ---- Precision: 1.0 / Recall: 0.791 / Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying GridSearchCV for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34.605725</td>\n",
       "      <td>0.459516</td>\n",
       "      <td>0.973954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.894188</td>\n",
       "      <td>0.116247</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37.250830</td>\n",
       "      <td>0.606872</td>\n",
       "      <td>0.973594</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>1.645170</td>\n",
       "      <td>0.077617</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.215421</td>\n",
       "      <td>0.394240</td>\n",
       "      <td>0.973415</td>\n",
       "      <td>0.999237</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.617557</td>\n",
       "      <td>0.039848</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.029916</td>\n",
       "      <td>0.620599</td>\n",
       "      <td>0.972337</td>\n",
       "      <td>0.993982</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.994385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.995061</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.993938</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.993938</td>\n",
       "      <td>0.876449</td>\n",
       "      <td>0.132181</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21.307609</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.972337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631862</td>\n",
       "      <td>0.075433</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "11      34.605725         0.459516         0.973954          1.000000   \n",
       "8       37.250830         0.606872         0.973594          0.999326   \n",
       "7       21.215421         0.394240         0.973415          0.999237   \n",
       "5       33.029916         0.620599         0.972337          0.993982   \n",
       "10      21.307609         0.470395         0.972337          1.000000   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "11            None                300   \n",
       "8               90                300   \n",
       "7               90                150   \n",
       "5               60                300   \n",
       "10            None                150   \n",
       "\n",
       "                                      params  rank_test_score  \\\n",
       "11  {'max_depth': None, 'n_estimators': 300}                1   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}                2   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}                3   \n",
       "5     {'max_depth': 60, 'n_estimators': 300}                4   \n",
       "10  {'max_depth': None, 'n_estimators': 150}                4   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "11           0.976682            1.000000       ...                  0.977538   \n",
       "8            0.979372            0.999551       ...                  0.972147   \n",
       "7            0.980269            0.999326       ...                  0.972147   \n",
       "5            0.978475            0.994385       ...                  0.971249   \n",
       "10           0.977578            1.000000       ...                  0.973944   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "11            1.000000           0.968553            1.000000   \n",
       "8             0.999102           0.968553            0.999551   \n",
       "7             0.999326           0.970350            0.999326   \n",
       "5             0.995061           0.967655            0.993938   \n",
       "10            1.000000           0.966757            1.000000   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "11           0.973046            1.000000      4.894188        0.116247   \n",
       "8            0.971249            0.999102      1.645170        0.077617   \n",
       "7            0.970350            0.999102      0.617557        0.039848   \n",
       "5            0.969452            0.993938      0.876449        0.132181   \n",
       "10           0.971249            1.000000      0.631862        0.075433   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "11        0.003171         0.000000  \n",
       "8         0.003891         0.000201  \n",
       "7         0.003680         0.000110  \n",
       "5         0.003884         0.000808  \n",
       "10        0.003534         0.000000  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10,150,300],\n",
    "         'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.642415</td>\n",
       "      <td>0.668842</td>\n",
       "      <td>0.975031</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>2.420369</td>\n",
       "      <td>0.071214</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22.887506</td>\n",
       "      <td>0.527546</td>\n",
       "      <td>0.973954</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973944</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.998877</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.999102</td>\n",
       "      <td>1.768570</td>\n",
       "      <td>0.202324</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45.441950</td>\n",
       "      <td>0.519678</td>\n",
       "      <td>0.973954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.475327</td>\n",
       "      <td>0.146532</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.808788</td>\n",
       "      <td>0.633799</td>\n",
       "      <td>0.973594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541772</td>\n",
       "      <td>0.204343</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.324336</td>\n",
       "      <td>0.340244</td>\n",
       "      <td>0.971978</td>\n",
       "      <td>0.993623</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.994609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.993714</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.993489</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.993938</td>\n",
       "      <td>0.389972</td>\n",
       "      <td>0.054211</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.000732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "8       40.642415         0.668842         0.975031          0.999326   \n",
       "7       22.887506         0.527546         0.973954          0.999282   \n",
       "11      45.441950         0.519678         0.973954          1.000000   \n",
       "10      25.808788         0.633799         0.973594          1.000000   \n",
       "4       15.324336         0.340244         0.971978          0.993623   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "8               90                300   \n",
       "7               90                150   \n",
       "11            None                300   \n",
       "10            None                150   \n",
       "4               60                150   \n",
       "\n",
       "                                      params  rank_test_score  \\\n",
       "8     {'max_depth': 90, 'n_estimators': 300}                1   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}                2   \n",
       "11  {'max_depth': None, 'n_estimators': 300}                2   \n",
       "10  {'max_depth': None, 'n_estimators': 150}                4   \n",
       "4     {'max_depth': 60, 'n_estimators': 150}                5   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "8            0.979372            0.999326       ...                  0.975741   \n",
       "7            0.978475            0.999551       ...                  0.973944   \n",
       "11           0.976682            1.000000       ...                  0.975741   \n",
       "10           0.976682            1.000000       ...                  0.977538   \n",
       "4            0.975785            0.994609       ...                  0.971249   \n",
       "\n",
       "    split2_train_score  split3_test_score  split3_train_score  \\\n",
       "8             0.999102           0.969452            0.999775   \n",
       "7             0.999551           0.966757            0.998877   \n",
       "11            1.000000           0.966757            1.000000   \n",
       "10            1.000000           0.968553            1.000000   \n",
       "4             0.993714           0.967655            0.993489   \n",
       "\n",
       "    split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "8            0.975741            0.998877      2.420369        0.071214   \n",
       "7            0.973046            0.999102      1.768570        0.202324   \n",
       "11           0.972147            1.000000      9.475327        0.146532   \n",
       "10           0.972147            1.000000      0.541772        0.204343   \n",
       "4            0.970350            0.993938      0.389972        0.054211   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "8         0.003195         0.000318  \n",
       "7         0.004145         0.000262  \n",
       "11        0.004142         0.000000  \n",
       "10        0.003252         0.000000  \n",
       "4         0.002986         0.000732  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10,150,300],\n",
    "         'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_count_feat, data['label'])\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build our own Grid-Search ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_GB(n_est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support= score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print ('Est.: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, \n",
    "        max_depth, \n",
    "        lr,\n",
    "        round(precision, 3),\n",
    "        round(recall, 3),\n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est.: 50 / Depth: None / LR: 0.1 ---- Precision: 0.906 / Recall: 0.851 / Accuracy: 0.969\n",
      "Est.: 50 / Depth: None / LR: 1 ---- Precision: 0.901 / Recall: 0.858 / Accuracy: 0.969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dbe1ef1276c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mtrain_GB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-2d368988b37b>\u001b[0m in \u001b[0;36mtrain_GB\u001b[0;34m(n_est, max_depth, lr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_GB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py3k/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3,7,11,15]:\n",
    "        for lr in [0.1, 1]:\n",
    "            train_GB(n_est, depth, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
